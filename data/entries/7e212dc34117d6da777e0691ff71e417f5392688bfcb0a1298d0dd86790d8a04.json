{"title":"Scale is All You Need?","link":"https://www.usv.com/writing/2024/05/scale-is-all-you-need/","date":1715115475000,"content":"<figure><img width=\"1024\" height=\"585\" src=\"https://www.usv.com/wp-content/uploads/2024/05/DALL·E-2024-05-07-14.07.24-Create-a-728x90px-banner-for-a-blog-post-on-AI-advancements.-The-design-should-visually-interpret-the-complex-and-cutting-edge-nature-of-artificial-in-1024x585.webp\" srcset=\"https://www.usv.com/wp-content/uploads/2024/05/DALL·E-2024-05-07-14.07.24-Create-a-728x90px-banner-for-a-blog-post-on-AI-advancements.-The-design-should-visually-interpret-the-complex-and-cutting-edge-nature-of-artificial-in-1024x585.webp 1024w, https://www.usv.com/wp-content/uploads/2024/05/DALL·E-2024-05-07-14.07.24-Create-a-728x90px-banner-for-a-blog-post-on-AI-advancements.-The-design-should-visually-interpret-the-complex-and-cutting-edge-nature-of-artificial-in-300x171.webp 300w, https://www.usv.com/wp-content/uploads/2024/05/DALL·E-2024-05-07-14.07.24-Create-a-728x90px-banner-for-a-blog-post-on-AI-advancements.-The-design-should-visually-interpret-the-complex-and-cutting-edge-nature-of-artificial-in-768x439.webp 768w, https://www.usv.com/wp-content/uploads/2024/05/DALL·E-2024-05-07-14.07.24-Create-a-728x90px-banner-for-a-blog-post-on-AI-advancements.-The-design-should-visually-interpret-the-complex-and-cutting-edge-nature-of-artificial-in-1536x878.webp 1536w, https://www.usv.com/wp-content/uploads/2024/05/DALL·E-2024-05-07-14.07.24-Create-a-728x90px-banner-for-a-blog-post-on-AI-advancements.-The-design-should-visually-interpret-the-complex-and-cutting-edge-nature-of-artificial-in-45x26.webp 45w, https://www.usv.com/wp-content/uploads/2024/05/DALL·E-2024-05-07-14.07.24-Create-a-728x90px-banner-for-a-blog-post-on-AI-advancements.-The-design-should-visually-interpret-the-complex-and-cutting-edge-nature-of-artificial-in-168x96.webp 168w, https://www.usv.com/wp-content/uploads/2024/05/DALL·E-2024-05-07-14.07.24-Create-a-728x90px-banner-for-a-blog-post-on-AI-advancements.-The-design-should-visually-interpret-the-complex-and-cutting-edge-nature-of-artificial-in-1568x896.webp 1568w, https://www.usv.com/wp-content/uploads/2024/05/DALL·E-2024-05-07-14.07.24-Create-a-728x90px-banner-for-a-blog-post-on-AI-advancements.-The-design-should-visually-interpret-the-complex-and-cutting-edge-nature-of-artificial-in.webp 1792w\" /></figure>\n\n\n\n<p>Progress in artificial intelligence has come in the form of a bag of tricks, some of which have proven to be unreasonably effective: word embeddings as a way of representing semantic similarity; the attention mechanism for contextualization; self-supervised learning on curated datasets; instruction tuning of models; reinforcement learning based on human feedback. The key to making these tricks “unreasonably effective” has turned out to be scale, such as taking the number of parameters in models up by several orders of magnitude.</p>\n\n\n\n<p>Despite the amazing recent progress, many applications will require further step function improvements in accuracy, reliability and creativity. Is even more scale all we need? This is one way to read <a href=\"https://en.wikipedia.org/wiki/Richard_S._Sutton\">Rich Sutton</a>’s famous “<a href=\"http://www.incompleteideas.net/IncIdeas/BitterLesson.html\">The Bitter Lesson</a>” essay. Will more scale give us the levels of accuracy and reliability, including absence of hallucinations, required for legal work or medical advice? Will more scale provide the creativity to create new scientific knowledge, such as novel proofs? </p>\n\n\n\n<p>Or might we need some more tricks. And would those be entirely new tricks or ones we already know but haven’t fully integrated yet into our current bag. Another way to read the essay is simply as an exhortation to look for methods that can scale arbitrarily with computation (Sutton calls out search and learning). Of course it could also turn out that we need some tricks that don’t rely on scaling in quite the same way or at all.</p>\n\n\n\n<p>Here is a list of known candidates that seem promising:</p>\n\n\n\n<p>1. Learning in simulated environments</p>\n\n\n\n<p>Humans learn a lot from living in the world. Robots exploring real or simulated worlds are learning more about causality. Simulation is already being used extensively in training models for autonomous vehicles. There is also work on more general simulation, such as <a href=\"https://www.threedworld.org/\">ThreeDWorld</a> out of MIT (<a href=\"https://dicarlolab.mit.edu/threedworld-platform-interactive-multi-modal-physical-simulation\">original paper</a> from 2020). Simulation can scale arbitrarily with computation and thus fits with Sutton’s conclusion. A recent fun and compelling result by <a href=\"https://jimfan.me/\">Jim Fan</a> is a <a href=\"https://twitter.com/DrJimFan/status/1786429467537088741\">robot dog walking on top of an exercise ball</a> simply based on simulation.</p>\n\n\n\n<p>2. Multi-agent learning</p>\n\n\n\n<p>Much of human behavior and learning arises from interactions with other humans. Multiagent environments are showing promise for similar effects among models, as recently demonstrated in a sandbox inspired by The Sims (<a href=\"https://arxiv.org/abs/2304.03442\">paper</a> and <a href=\"https://github.com/joonspk-research/generative_agents\">code</a>). Multi agent learning too can keep scaling with computation and also leverage simulation.</p>\n\n\n\n<p>3. Reliable use of tools</p>\n\n\n\n<p>One of the earliest departures for humans from (most) other species comes in our use of tools. Today very few people would attempt multiplying large numbers in their head. We would use a calculator instead. A promising direction is training models to reliably know when and how to use such tools (see for example the <a href=\"https://arxiv.org/abs/2302.04761\">Toolformer paper</a>). This feels less like a trick that can scale with computation but rather a way to reliably leverage all of our existing investment in computation.</p>\n\n\n\n<p>4. Explicit structured knowledge</p>\n\n\n\n<p>Prior to the breakthroughs with LLMs a lot of work had gone into explicit knowledge representations in the form of ontologies, knowledge graphs, rules-based-systems and the like. Accessing those kinds of systems could be seen as a specialized case of tool use but also seems to have additional potential. UnifiedSKG is one way to bring a lot of so-called structured knowledge grounding techniques into modes (<a href=\"https://arxiv.org/abs/2201.05966\">paper</a> and <a href=\"https://github.com/xlang-ai/UnifiedSKG\">code</a>). Sutton explicitly called out attempts to build knowledge into agents as having been outperformed by statistical methods. But human-machine collaboration might dramatically grow the extent of explicit knowledge.</p>\n\n\n\n<figure><div>\n\n</div><figcaption>An “unreasonably effective” AI generated video**</figcaption></figure>\n\n\n\n<p>These are just four examples of tricks that we already know and that are being actively researched. There is a ton more to explore, including the effects of adding different types of <a href=\"https://arxiv.org/abs/2404.11672\">memory</a>, bringing in techniques from <a href=\"https://arxiv.org/abs/2312.14135\">search</a>, or wrapping calls to models in <a href=\"https://github.com/Codium-ai/AlphaCodium\">workflows</a>.</p>\n\n\n\n<p>All of this research is exciting because of the significant gains achieved by these “known tricks” even in early and still relatively crude versions. This progress raises the tantalizing possibility that we can get much further with current and next generation open source models (*). With the right “core” there could be an ecosystem akin to what we have with open source operating systems (that’s one way to interpret Andrej Karpathy’s <a href=\"https://twitter.com/karpathy/status/1723140519554105733\">tweet about an LLM OS</a>).</p>\n\n\n\n<p>Now it is also still possible that we will encounter some fundamental limitation with autoregressive models as the core building block. That would mean we need new tricks, some of which might be quite fundamental. We may, for example, have to find more complex objective functions for pre-training. That could work in the opposite direction where training might become more expensive.</p>\n\n\n\n<p>Why does all of this matter? From a market structure and innovation perspective it would be fantastic if the cost of training runs doesn’t continue to explode, which would heavily favor a highly concentrated market dominated by a few closed source systems (they would be closed source to recoup the exploding cost of the training runs). </p>\n\n\n\n<p>We are excited about our <a href=\"https://www.usv.com/companies/?industry-cat=ai\">existing AI portfolio</a> including <a href=\"https://www.hume.ai/\">Hume</a>, <a href=\"https://www.abridge.com/\">Abridge</a>, <a href=\"https://www.clarifai.com/\">Clarifai</a>, <a href=\"https://casetext.com/\">Casetext</a> (already exited), and several other recent investments we have yet to announce. Based on the above, we also believe that a lot remains to be figured out. We are partial to composable open systems where these “tricks” don’t need to all be figured out by one company but can be contributed to by many. These systems might well be specific to a problem set, such as medical diagnosis or materials discovery. </p>\n\n\n\n<p>* There have been a lot of important arguments from a safety and x-risk perspective against pushing open source systems further. This post can’t possibly address these but it is worth noting that many of the tricks mentioned above might allow the core system to do less, which would increase safety and reduce risk.</p>\n\n\n\n<p>**The above video was made with almost no human input. We fed this blog post into Claude and used a single prompt to develop a corresponding trailer with a 30 second script, style guidelines, formatted b roll prompts, and a music track prompt. The results were copy and pasted into RunwayML, Suno, and Eleven Labs and then stitched together without edit. We took it upon ourselves to add a title at the end.</p>\n<p>The post <a href=\"https://www.usv.com/writing/2024/05/scale-is-all-you-need/\">Scale is All You Need?</a> appeared first on <a href=\"https://www.usv.com\">Union Square Ventures</a>.</p>","author":"Albert Wenger","siteTitle":"http://www.usv.com/feed","siteHash":"628cf95acf2204bb0b1633ad294b90483a47fc067742fc28c14bbe7ca7e45bd8","entryHash":"7e212dc34117d6da777e0691ff71e417f5392688bfcb0a1298d0dd86790d8a04","category":"default"}