{"title":"Info-Sec: Adversarial Machine Learning","link":"https://ghuneim.com/2024/01/06/info-sec-adversarial-machine-learning/","date":1704555876000,"content":"<h1><a href=\"https://www.nist.gov/news-events/news/2024/01/nist-identifies-types-cyberattacks-manipulate-behavior-ai-systems\">NIST Identifies Types of Cyberattacks That Manipulate Behavior of AI Systems</a></h1>\n<div><img src=\"https://ghuneim.com/wp-content/uploads/2024/01/AI-evasion-rev-3-650x433.png\" alt=\"Adversarial Machine Learning\" width=\"650\" height=\"433\" srcset=\"https://ghuneim.com/wp-content/uploads/2024/01/AI-evasion-rev-3-650x433.png 650w, https://ghuneim.com/wp-content/uploads/2024/01/AI-evasion-rev-3-1300x867.png 1300w, https://ghuneim.com/wp-content/uploads/2024/01/AI-evasion-rev-3-325x217.png 325w\" /><p>An AI system can malfunction if an adversary finds a way to confuse its decision making. In this example, errant markings on the road mislead a driverless car, potentially making it veer into oncoming traffic. This “evasion” attack is one of numerous adversarial tactics described in a new NIST publication intended to help outline the types of attacks we might expect along with approaches to mitigate them.</p></div>\n<p><a href=\"https://csrc.nist.gov/pubs/ai/100/2/e2023/final\"><em>Adversarial Machine Learning: A Taxonomy and Terminology of Attacks and Mitigations</em> (NIST.AI.100-2)</a>, 106 page PDF. Part of NIST <a href=\"https://www.nist.gov/artificial-intelligence/executive-order-safe-secure-and-trustworthy-artificial-intelligence\">trustworthy AI</a> initiative.</p>\n<p>The report considers the four major types of attacks: evasion, poisoning, privacy and abuse attacks. It also classifies them according to multiple criteria such as the attacker’s goals and objectives, capabilities, and knowledge.</p>\n<p><strong>Evasion</strong> attacks, which occur after an AI system is deployed, attempt to alter an input to change how the system responds to it. Examples would include adding markings to stop signs to make an autonomous vehicle misinterpret them as speed limit signs or creating confusing lane markings to make the vehicle veer off the road.</p>\n<p><strong>Poisoning</strong> attacks occur in the training phase by introducing corrupted data. An example would be slipping numerous instances of inappropriate language into conversation records, so that a chatbot interprets these instances as common enough parlance to use in its own customer interactions.</p>\n<p><strong>Privacy</strong> attacks, which occur during deployment, are attempts to learn sensitive information about the AI or the data it was trained on in order to misuse it. An adversary can ask a chatbot numerous legitimate questions, and then use the answers to reverse engineer the model so as to find its weak spots — or guess at its sources. Adding undesired examples to those online sources could make the AI behave inappropriately, and making the AI unlearn those specific undesired examples after the fact can be difficult.</p>\n<p><strong>Abuse</strong> attacks involve the insertion of incorrect information into a source, such as a webpage or online document, that an AI then absorbs. Unlike the aforementioned poisoning attacks, abuse attacks attempt to give the AI incorrect pieces of information from a legitimate but compromised source to repurpose the AI system’s intended use.</p>\n<p> </p>","author":"MDG","siteTitle":"Mark Ghuneim","siteHash":"fbdbfb355dab5247b3a3d9c04401313e99252873a1ae0fb1ea729c2e5736c6f3","entryHash":"b08ca2df05cd6385d14ad7ec7933dc863c819fb78ea1f179797fa3d61405e900","category":"default"}